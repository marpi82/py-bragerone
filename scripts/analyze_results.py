#!/usr/bin/env python3
"""Analysis script for live test results.

This script analyzes the JSON files generated by test_live_menu.py
and provides insights into the menu parsing and permission filtering.

Usage:
    poetry run python scripts/analyze_results.py
"""

import json
import logging
import os
from pathlib import Path

from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()


def setup_logging() -> logging.Logger:
    """Set up logging configuration."""
    log_level = os.getenv("LOG_LEVEL", "INFO").upper()
    logging.basicConfig(level=getattr(logging, log_level), format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    return logging.getLogger("analyze")


def analyze_test_results() -> None:
    """Analyze all live test result files."""
    logger = setup_logging()

    print("🔍 Live Test Results Analysis")
    print("=" * 50)

    # Find all result files
    results_dir = Path("scripts/live_test_results")
    if not results_dir.exists():
        print("❌ No test result directory found!")
        print("Make sure to run 'poetry run python scripts/test_live_menu.py' first.")
        return

    result_files = list(results_dir.glob("live_test_results_device_*.json"))

    if not result_files:
        print("❌ No test result files found!")
        print("Make sure to run 'poetry run python scripts/test_live_menu.py' first.")
        return

    print(f"📄 Found {len(result_files)} result files:")

    for file_path in sorted(result_files):
        print(f"\n📋 Analyzing: {file_path.name}")
        print("-" * 30)

        try:
            with open(file_path, encoding="utf-8") as f:
                data = json.load(f)

            # Basic info
            device_menu = data["device_menu"]
            obj_info = data["object_info"]
            permissions = data["permissions"]
            test_info = data.get("test_info", {})

            print(f"🎯 Device Menu: {device_menu}")
            print(f"🏢 Object: {obj_info['name']} (ID: {obj_info['id']})")

            user_perms = permissions.get("user_permissions", [])
            obj_perms = permissions.get("object_permissions", [])
            print(f"🔒 User Permissions: {len(user_perms)} total")
            print(f"🔒 Object Permissions: {len(obj_perms)} total")

            if test_info:
                print(f"📅 Test User: {test_info.get('user_email', 'N/A')}")
                print(f"📅 Log Level: {test_info.get('log_level', 'N/A')}")

            # Menu analysis
            menu_all = data["menu_all"]
            menu_filtered = data["menu_filtered"]
            security = data["security_analysis"]

            print("\n📊 Menu Structure:")
            print(f"  - Routes (unfiltered): {menu_all['routes_count']}")
            print(f"  - Routes (filtered): {menu_filtered['routes_count']}")
            print(f"  - Total tokens: {security['total_tokens']}")
            print(f"  - Visible tokens: {security['visible_tokens']}")
            print(f"  - Hidden tokens: {security['hidden_tokens']}")

            # Security analysis
            filter_efficiency = security.get("filter_efficiency_percent", 0)
            if security["hidden_tokens"] > 0:
                print("\n🔐 Security Filter Active:")
                print(f"  - {security['hidden_tokens']} tokens hidden by permissions")
                print(f"  - Filter efficiency: {filter_efficiency}%")

                # Show some hidden tokens
                hidden_list = security["hidden_token_list"]
                if hidden_list:
                    print(f"  - Sample hidden tokens: {', '.join(hidden_list[:5])}")
                    if len(hidden_list) > 5:
                        print(f"    ... and {len(hidden_list) - 5} more")

                # Security assessment
                if filter_efficiency > 20:
                    print("  ✅ Good security filtering (>20% tokens hidden)")
                elif filter_efficiency > 5:
                    print("  ⚠️  Moderate security filtering (5-20% tokens hidden)")
                else:
                    print("  ⚠️  Low security filtering (<5% tokens hidden)")

            else:
                print("\n⚠️  No security filtering detected!")
                print("  - All tokens are visible - this may be a security concern")
                print("  - Consider checking permission configuration")

            # Sample route structure
            if menu_filtered.get("sample_route"):
                route = menu_filtered["sample_route"]
                print("\n🗂️  Sample Route Structure:")
                print(f"  - Path: {route.get('path', 'N/A')}")
                print(f"  - Name: {route.get('name', 'N/A')}")
                if route.get("children"):
                    print(f"  - Children: {len(route['children'])} sub-routes")
                if route.get("params"):
                    print(f"  - Parameters: {len(route['params'])}")
                if route.get("gate"):
                    print(f"  - Security Gate: {route.get('gate')}")

            # Comparison suggestions
            print("\n✅ Manual Verification Suggestions:")
            print("  1. Login to BragerOne app with credentials from .env")
            print(f"  2. Navigate to object: {obj_info['name']}")
            print(f"  3. Check if you can see all {security['visible_tokens']} visible parameters")
            print("  4. Verify that hidden parameters are indeed not accessible")

        except Exception as e:
            logger.error("❌ Error analyzing %s: %s", file_path, e)


def compare_device_menus() -> None:
    """Compare results between different device_menu values."""
    print("\n🔄 Device Menu Comparison")
    print("=" * 30)

    results_dir = Path("scripts/live_test_results")
    if not results_dir.exists():
        print("i  No results directory found")
        return

    # Load all files
    results = {}
    for file_path in results_dir.glob("live_test_results_device_*.json"):
        try:
            with open(file_path, encoding="utf-8") as f:
                data = json.load(f)
                results[data["device_menu"]] = data
        except Exception as e:
            print(f"❌ Error loading {file_path}: {e}")

    if len(results) < 2:
        print("i  Need at least 2 device_menu results to compare")
        return

    # Compare results
    for device1 in sorted(results.keys()):
        for device2 in sorted(results.keys()):
            if device1 >= device2:
                continue

            data1 = results[device1]
            data2 = results[device2]

            print(f"\n📊 Comparing device_menu {device1} vs {device2}:")

            # Token differences
            tokens1 = set(data1["menu_filtered"]["tokens"])
            tokens2 = set(data2["menu_filtered"]["tokens"])

            common = tokens1 & tokens2
            only1 = tokens1 - tokens2
            only2 = tokens2 - tokens1

            print(f"  - Common tokens: {len(common)}")
            print(f"  - Only in {device1}: {len(only1)}")
            print(f"  - Only in {device2}: {len(only2)}")

            if only1:
                print(f"    Unique to {device1}: {', '.join(list(only1)[:3])}{'...' if len(only1) > 3 else ''}")
            if only2:
                print(f"    Unique to {device2}: {', '.join(list(only2)[:3])}{'...' if len(only2) > 3 else ''}")

            # Route differences
            routes1 = data1["menu_filtered"]["routes_count"]
            routes2 = data2["menu_filtered"]["routes_count"]
            print(f"  - Routes: {routes1} vs {routes2} ({abs(routes1 - routes2)} difference)")

            # Security comparison
            sec1 = data1["security_analysis"]
            sec2 = data2["security_analysis"]
            eff1 = sec1.get("filter_efficiency_percent", 0)
            eff2 = sec2.get("filter_efficiency_percent", 0)
            print(f"  - Filter efficiency: {eff1}% vs {eff2}% ({abs(eff1 - eff2):.1f}% difference)")


def generate_summary_report() -> None:
    """Generate a comprehensive summary report."""
    print("\n📋 Summary Report")
    print("=" * 20)

    results_dir = Path("scripts/live_test_results")
    if not results_dir.exists():
        print("i  No results to summarize")
        return

    result_files = list(results_dir.glob("live_test_results_device_*.json"))
    if not result_files:
        print("i  No results to summarize")
        return

    print(f"📊 Total test runs: {len(result_files)}")

    total_tokens = 0
    total_visible = 0
    total_hidden = 0

    for file_path in result_files:
        try:
            with open(file_path, encoding="utf-8") as f:
                data = json.load(f)
                security = data["security_analysis"]
                total_tokens += security["total_tokens"]
                total_visible += security["visible_tokens"]
                total_hidden += security["hidden_tokens"]
        except Exception:
            continue

    if total_tokens > 0:
        avg_filter_efficiency = (total_hidden / total_tokens) * 100
        print(f"🔒 Average filter efficiency: {avg_filter_efficiency:.1f}%")
        print(f"📊 Total unique tokens analyzed: {total_tokens}")
        print(f"👁️  Total visible tokens: {total_visible}")
        print(f"🔒 Total hidden tokens: {total_hidden}")

        # Security assessment
        if avg_filter_efficiency > 20:
            print("✅ Overall security assessment: GOOD")
        elif avg_filter_efficiency > 5:
            print("⚠️  Overall security assessment: MODERATE")
        else:
            print("❌ Overall security assessment: NEEDS ATTENTION")


def main() -> None:
    """Main analysis function."""
    analyze_test_results()
    compare_device_menus()
    generate_summary_report()

    print("\n🎯 Next Steps:")
    print("1. Review the analysis above")
    print("2. Login to your BragerOne app")
    print("3. Compare app interface with parsed results")
    print("4. Report any discrepancies or security issues")
    print("5. Check scripts/live_test_results/ for detailed JSON files")


if __name__ == "__main__":
    main()
